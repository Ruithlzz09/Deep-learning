{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and Output layer -- 2 Layer MLP Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Activation Function -- Sigmoid function\n",
    "def sigmoid(x,deriv=False):\n",
    "    if (deriv==False):\n",
    "        return x*(1-x)\n",
    "    return 1.0/(1+np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "x=np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "y=np.array([ [0,0,1,1]]).T\n",
    "\n",
    "#random seed generation to have same deterministic value at each time during train\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete\n",
      "\n",
      "[[ -8.19493377]\n",
      " [-24.44251364]\n",
      " [-17.49746021]\n",
      " [-39.13708231]]\n"
     ]
    }
   ],
   "source": [
    "#Initializing Weights\n",
    "syn0=2*np.random.random((3,1)) - 1\n",
    "\n",
    "for iter in range(100):\n",
    "    \n",
    "    #forward propagation\n",
    "    l0=x\n",
    "    l1=sigmoid(np.dot(l0,syn0)) #Output layer\n",
    "    \n",
    "    #Backpropagation\n",
    "    l1_error=y-l1\n",
    "    \n",
    "    l1_delta=l1_error*sigmoid(l1,deriv=True)\n",
    "    #update Weights\n",
    "    syn0+=np.dot(l0.T,l1_delta)\n",
    "\n",
    "print(\"Training Complete\",end='\\n\\n')\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different kinds of parameters can be adjusted during the training of an ANN, the weights and the value in the activation functions. This is impractical and it would be easier if only one of the parameters should be adjusted. To cope with this problem a bias neuron is invented. The bias neuron lies in one layer, is connected to all the neurons in the next layer, but none in the previous layer and it always emits 1. Since the bias neuron emits 1 the weights, connected to the bias neuron, are added directly to the combined sum of the other weights (equation 2.1), just like the t value in the activation functions.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Layer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Activation Function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "            [1],\n",
    "            [1],\n",
    "            [0]])\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Weight Initialization\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.4964100319027255\n",
      "Error:0.008584525653247157\n",
      "Error:0.005789459862507806\n",
      "Error:0.004629176776769983\n",
      "Error:0.0039587652802736475\n",
      "Error:0.0035101225678616744\n",
      "\n",
      "\n",
      "Training Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(60000):\n",
    "\n",
    "    # Feed-forward through layers 0, 1, and 2\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    "\n",
    "    # Back-propagation\n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if (j% 10000) == 0:\n",
    "        print (\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "    \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
    "\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "\n",
    "    \n",
    "    #Weights Updation\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "print(\"\\n\\nTraining Complete\",end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
